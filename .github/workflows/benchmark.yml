name: Benchmark Regression Analysis

on:
    workflow_dispatch:
        inputs:
            target_ref:
                description: 'SHA commit to benchmark.'
                required: false
                default: ''
                type: string
            
            benchmark_type:
                description: 'Type of benchmark to run'
                required: true
                default: 'simple'
                type: choice
                options:
                    - 'full'
                    - 'simple'
            
            filter:
                description: 'Filter tests by name pattern (eg. "TestPartialTraceBenchmarks")'
                required: false
                default: "TestPartialTraceBenchmarks"
                type: string
            
            function:
                description: 'Filter tests by function pattern (eg. "test_bench__partial_trace__vary__dim")'
                required: false
                default: "test_bench__partial_trace__vary__dim"
                type: string
            
            save_results:
                description: 'Save benchmark results to storage'
                required: false
                default: true
                type: boolean
            
            fail_on_regression:
                description: 'Fail the workflow if regressions are detected'
                required: false
                default: true
                type: boolean
            
            regression_threshold:
                description: 'Regression threshold percentage (e.g., 10 for 10%)'
                required: false
                default: '10'
                type: string
            
    pull_request:
        branches:
            - master

env:
    PYTHON_VERSION: '3.11'
    BASE_BENCHMARK_FILE: "benchmarks/baseline.json"
    BENCHMARK_FILE_TOQITO: "toqito-bench/scripts/benchmark_toqito.py"
    BENCHMARK_STORAGE: "results"
    BENCHMARK_DIR: "scripts"


jobs:
  benchmark-regression:
    runs-on: ubuntu-latest
    
    #steps:
    # - name: Determine target reference
    #   id: target-ref
    #   run: |
    #     if [ -n "${{ github.event.inputs.target_ref }}" ]; then
    #       echo "ref=${{ github.event.inputs.target_ref }}" >> $GITHUB_OUTPUT
    #       echo "Benchmarking specified reference: ${{ github.event.inputs.target_ref }}"
    #     else
    #       echo "ref=${{ github.sha }}" >> $GITHUB_OUTPUT
    #       echo "Benchmarking current commit: ${{ github.sha }}"
    #     fi

    steps:
    - name: Set default values for all trigger types
      id: defaults
      run: |

        echo "benchmark_type=${{ github.event.inputs.benchmark_type || 'simple' }}" >> $GITHUB_OUTPUT
        # echo "filter=${{ github.event.inputs.filter || '' }}" >> $GITHUB_OUTPUT
        # echo "function=${{ github.event.inputs.function || '' }}" >> $GITHUB_OUTPUT
        echo "filter=${{ github.event.inputs.filter || '' }}" >> $GITHUB_OUTPUT
        echo "function=${{ github.event.inputs.function || '' }}" >> $GITHUB_OUTPUT
        echo "save_results=${{ github.event.inputs.save_results || 'true' }}" >> $GITHUB_OUTPUT
        echo "fail_on_regression=${{ github.event.inputs.fail_on_regression || 'true' }}" >> $GITHUB_OUTPUT  
        echo "regression_threshold=${{ github.event.inputs.regression_threshold || '10' }}" >> $GITHUB_OUTPUT
        

        if [ -n "${{ github.event.inputs.target_ref }}" ]; then
          echo "target_ref=${{ github.event.inputs.target_ref }}" >> $GITHUB_OUTPUT
          echo " Using specified reference: ${{ github.event.inputs.target_ref }}"
        else
          echo "target_ref=${{ github.sha }}" >> $GITHUB_OUTPUT
          echo " Using current commit: ${{ github.sha }}"
        fi
        
        # Log the trigger type for debugging
        echo " Workflow triggered by: ${{ github.event_name }}"


    
    - name: Checkout toqito repository at the current/mentioned commit
      uses: actions/checkout@v4
      with:
        path: toqito
        ref: ${{ steps.defaults.outputs.target_ref }}
        fetch-depth: 0
    
    - name: Display benchmark target info
      run: |
        cd toqito
        echo "  BENCHMARK TARGET INFORMATION"
        echo "================================"
        echo "Repository: vprusso/toqito"
        echo "Reference: ${{ steps.defaults.outputs.target_ref }}"
        echo "Commit SHA: $(git rev-parse HEAD)"
        echo "Commit Message: $(git log -1 --pretty=format:'%s')"
        echo "Author: $(git log -1 --pretty=format:'%an <%ae>')"
        echo "Date: $(git log -1 --pretty=format:'%ad')"
        echo "Benchmark Type: ${{  steps.defaults.outputs.benchmark_type }}"
        echo "Filter Applied: ${{ steps.defaults.outputs.filter || 'none' }}"
        echo "Function Applied: ${{ steps.defaults.outputs.function || 'none' }}"
        echo "Save Results: ${{ steps.defaults.outputs.save_results }}"
        echo "Regression Threshold: ${{ steps.defaults.outputs.regression_threshold }}%"
        echo "Fail on Regression: ${{ steps.defaults.outputs.fail_on_regression  }}"
        echo "Triggered by: ${{ github.event_name }}"
        echo "================================"
    
    - name: Checkout toqito-bench repository  
      uses: actions/checkout@v4
      with:
        repository: vprusso/toqito-bench
        path: toqito-bench
        ref: post_cycle
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Poetry
      run: pip install poetry
    
    - name: Setup toqito environment
      run: |
        cd toqito
        echo "Setting up toqito development environment..."
        poetry install --with dev
        poetry add pytest-benchmark --group dev pytest-memray sympy pygal
        echo "Environment setup complete"
    
    - name: Verify baseline benchmark exists
      run: |
        BASE_FILE="toqito-bench/${{ env.BASE_BENCHMARK_FILE }}"
        if [ ! -f "$BASE_FILE" ]; then
          echo "  Baseline benchmark file not found at $BASE_FILE"
          echo "Please ensure the baseline benchmark exists in the toqito-bench repository"
          exit 1
        fi
        echo "  Baseline benchmark found !"
    
    - name: Prepare benchmark environment
      run: |
        echo "📁 Creating benchmark directories..."

        TIMESTAMP=$(date +%Y_%m_%d__%H_%M_%S)
        echo "TIMESTAMP=$TIMESTAMP" >> $GITHUB_ENV

        cd toqito-bench
        mkdir -p ${{ env.BENCHMARK_STORAGE }}/toqito/full
        mkdir -p ${{ env.BENCHMARK_STORAGE }}/toqito/simple

        FILTER="${{ steps.defaults.outputs.filter }}"
        FUNCTION="${{ steps.defaults.outputs.function }}"
        if [ -n "$FILTER" ] || [ -n "$FUNCTION" ]; then
          mkdir -p "${{ env.BENCHMARK_STORAGE }}/toqito/$FILTER/$FUNCTION"
        fi

        echo "📁 Directory structure created"
        ls -la ${{ env.BENCHMARK_STORAGE }}/toqito/
    
    - name: Run benchmark (full)
      if: steps.defaults.outputs.benchmark_type == 'full'
      run: |
        cd toqito-bench
        echo "Running FULL benchmarks for toqito..."
        echo "Benchmark file: scripts/benchmark_toqito.py"
        echo "Filter applied: ${{ steps.defaults.outputs.filter || 'none' }}"
        echo "Function applied: ${{ steps.defaults.outputs.function || 'none' }}"

        FILTER="${{ steps.defaults.outputs.filter }}"
        FUNCTION="${{ steps.defaults.outputs.function }}"
        SAVE="${{ steps.defaults.outputs.save_results }}"

        STORAGE_DIR="$(pwd)/${{ env.BENCHMARK_STORAGE }}/toqito/full"
        mkdir -p "$STORAGE_DIR"
        echo "💾 Storage: $STORAGE_DIR"

        export PYTHONPATH="../toqito:$PYTHONPATH"

        cd ../toqito

        PYTEST_CMD="poetry run pytest ../toqito-bench/scripts/benchmark_toqito.py"
        if [ -n "$FILTER" ]; then
          PYTEST_CMD="$PYTEST_CMD -k \"$FILTER\""
        fi
        if [ -n "$FUNCTION" ]; then
          PYTEST_CMD="$PYTEST_CMD -m \"$FUNCTION\""
        fi

        PYTEST_CMD="$PYTEST_CMD \
          --benchmark-warmup=on \
          --benchmark-sort=name \
          --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
          --benchmark-save=detailed_${{ env.TIMESTAMP }} \
          --benchmark-storage=$STORAGE_DIR \
          --benchmark-verbose \
          -v --tb=long"

        echo "🔧 Executing: $PYTEST_CMD currently in $(pwd)"
        eval "$PYTEST_CMD"

        echo "✅ Full benchmarks completed!"

        RESULTS_FILE=$(ls -1 "$STORAGE_DIR"/Linux-CPython-3.11-64bit/*detailed_${{ env.TIMESTAMP }}.json | head -n 1)
        RESULTS_FILE="$(realpath "$RESULTS_FILE")"
        echo "RESULTS_FILE=$RESULTS_FILE" >> $GITHUB_ENV
        echo "Using results file: $RESULTS_FILE"

    
    - name: Run benchmark (simple)
      if: steps.defaults.outputs.benchmark_type == 'simple'
      run: |
        cd toqito-bench
        echo "Running SIMPLE benchmarks for toqito..."
        echo "Benchmark file: ${{ env.BENCHMARK_FILE_TOQITO }}"
        echo "Filter applied: ${{ steps.defaults.outputs.filter || 'none' }}"
        echo "Function applied: ${{ steps.defaults.outputs.function || 'none' }}"

        FILTER="${{ steps.defaults.outputs.filter }}"
        FUNCTION="${{ steps.defaults.outputs.function }}"
        SAVE="${{ steps.defaults.outputs.save_results }}"

        if [ "$SAVE" = "true" ]; then
          STORAGE_DIR="$(pwd)/${{ env.BENCHMARK_STORAGE }}/toqito/$FILTER/$FUNCTION"
          mkdir -p "$STORAGE_DIR"
          echo "💾 Storage: $STORAGE_DIR"
        else
          echo "💾 Storage: not saving results"
        fi

        export PYTHONPATH="../toqito:$PYTHONPATH"

        cd ../toqito

        PYTEST_CMD="poetry run pytest ../toqito-bench/scripts/benchmark_toqito.py"
        if [ -n "$FILTER" ]; then
          PYTEST_CMD="$PYTEST_CMD -k \"$FILTER\""
        fi
        if [ -n "$FUNCTION" ]; then
          PYTEST_CMD="$PYTEST_CMD -m \"$FUNCTION\""
        fi

        PYTEST_CMD="$PYTEST_CMD \
          --benchmark-sort=name \
          --benchmark-columns=mean,median,stddev,ops \
          --tb=short"

        if [ "$SAVE" = "true" ]; then
          PYTEST_CMD="$PYTEST_CMD \
            --benchmark-save=simple_${{ env.TIMESTAMP }} \
            --benchmark-storage=$STORAGE_DIR"
        fi

        echo "🔧 Executing: $PYTEST_CMD currently in $(pwd)"
        eval "$PYTEST_CMD"

        echo "✅ Simple benchmarks completed!"

        if [ "$SAVE" = "true" ]; then
          RESULTS_FILE=$(ls -1 "$STORAGE_DIR"/Linux-CPython-3.11-64bit/*simple_${{ env.TIMESTAMP }}.json | head -n 1)
          # Make absolute path
          RESULTS_FILE="$(realpath "$RESULTS_FILE")"
          echo "RESULTS_FILE=$RESULTS_FILE" >> $GITHUB_ENV
          echo "Using results file: $RESULTS_FILE"
        else
          echo "RESULTS_FILE=" >> $GITHUB_ENV
        fi


        echo "📁 Benchmark directory structure:"
        BASE_DIR="../toqito-bench/${{ env.BENCHMARK_STORAGE }}/toqito"
        if [ -d "$BASE_DIR" ]; then
          find "$BASE_DIR" -type d -exec echo "Directory: {}" \;
          find "$BASE_DIR" -type f -exec echo "File: {}" \;
        else
          echo "No benchmark directory found at $BASE_DIR"
        fi

    # - name: Regression Analysis
    #   if: env.RESULTS_FILE != ''  # only run if we saved results
    #   run: |
    #     echo "📊 Running regression analysis..."
    #     BASE_FILE="$(pwd)/toqito-bench/${{ env.BASE_BENCHMARK_FILE }}"
    #     RESULTS_FILE="${{ env.RESULTS_FILE }}"  # already absolute
    #     THRESHOLD=${{ steps.defaults.outputs.regression_threshold }}
    #     FAIL_ON_REGRESSION=${{ steps.defaults.outputs.fail_on_regression }}

    #     echo "Baseline: $BASE_FILE"
    #     echo "Results:  $RESULTS_FILE"
    #     echo "Threshold: $THRESHOLD%"
    #     echo "Fail on Regression: $FAIL_ON_REGRESSION"

    #     python3 <<EOF
    #     import json, sys

    #     base_file = "${BASE_FILE}"
    #     results_file = "${RESULTS_FILE}"
    #     threshold = float("${THRESHOLD}")
    #     fail_on_regression = "${FAIL_ON_REGRESSION}".lower() == "true"

    #     with open(base_file) as f:
    #         base = json.load(f)
    #     with open(results_file) as f:
    #         results = json.load(f)

    #     regressions = []
    #     indicators = {}

    #     # pytest-benchmark stores results in list-of-dicts format
    #     base_map = {b["name"]: b for b in base["benchmarks"]}
    #     for r in results["benchmarks"]:
    #         name = r["name"]
    #         if name not in base_map:
    #             continue
    #         base_mean = base_map[name]["stats"]["mean"]
    #         curr_mean = r["stats"]["mean"]
    #         change_pct = ((curr_mean - base_mean) / base_mean) * 100

    #         indicators[name] = {
    #             "baseline": base_mean,
    #             "current": curr_mean,
    #             "change_pct": change_pct,
    #             "regression": change_pct > threshold
    #         }

    #         if change_pct > threshold:
    #             regressions.append((name, base_mean, curr_mean, change_pct))

    #     print("📈 Regression Analysis Report")
    #     for name, data in indicators.items():
    #         print(f"- {name}: baseline={data['baseline']:.6f}, "
    #               f"current={data['current']:.6f}, "
    #               f"Δ={data['change_pct']:.2f}% "
    #               f"{'⚠️ REGRESSION' if data['regression'] else '✅ OK'}")

    #     if regressions:
    #         print("\n❌ Regressions detected:")
    #         for n, b, c, pct in regressions:
    #             print(f"  {n}: {b:.6f} → {c:.6f} ({pct:.2f}% slower)")
    #         if fail_on_regression:
    #             sys.exit(1)
    #     else:
    #         print("✅ No regressions found")
    #     EOF
    - name: Regression Analysis
      if: env.RESULTS_FILE != ''  # only run if we have a results file
      run: |
        echo "📊 Running regression analysis..."
        
        # Absolute paths
        BASE_FILE="$(realpath toqito-bench/${{ env.BASE_BENCHMARK_FILE }})"
        RESULTS_FILE="$(realpath "${{ env.RESULTS_FILE }}")"
        
        THRESHOLD=${{ steps.defaults.outputs.regression_threshold }}
        FAIL_ON_REGRESSION=${{ steps.defaults.outputs.fail_on_regression }}

        echo "Baseline: $BASE_FILE"
        echo "Results:  $RESULTS_FILE"
        echo "Threshold: $THRESHOLD%"
        echo "Fail on Regression: $FAIL_ON_REGRESSION"

        python3 <<EOF
        import json, sys, os

        base_file = "${BASE_FILE}"
        results_file = "${RESULTS_FILE}"
        threshold = float("${THRESHOLD}")
        fail_on_regression = "${FAIL_ON_REGRESSION}".lower() == "true"

        # Validate files exist
        if not os.path.isfile(base_file):
            print(f"❌ Baseline file not found: {base_file}")
            sys.exit(1)
        if not os.path.isfile(results_file):
            print(f"❌ Results file not found: {results_file}")
            sys.exit(1)

        with open(base_file) as f:
            base = json.load(f)
        with open(results_file) as f:
            results = json.load(f)

        regressions = []
        indicators = {}

        # Map benchmarks by name for comparison
        base_map = {b["name"]: b for b in base.get("benchmarks", [])}
        for r in results.get("benchmarks", []):
            name = r["name"]
            if name not in base_map:
                continue
            base_mean = base_map[name]["stats"]["mean"]
            curr_mean = r["stats"]["mean"]
            change_pct = ((curr_mean - base_mean) / base_mean) * 100

            indicators[name] = {
                "baseline": base_mean,
                "current": curr_mean,
                "change_pct": change_pct,
                "regression": change_pct > threshold
            }

            if change_pct > threshold:
                regressions.append((name, base_mean, curr_mean, change_pct))

        print("📈 Regression Analysis Report")
        for name, data in indicators.items():
            print(f"- {name}: baseline={data['baseline']:.6f}, "
                  f"current={data['current']:.6f}, "
                  f"Δ={data['change_pct']:.2f}% "
                  f"{'⚠️ REGRESSION' if data['regression'] else '✅ OK'}")

        if regressions:
            print("\n❌ Regressions detected:")
            for n, b, c, pct in regressions:
                print(f"  {n}: {b:.6f} → {c:.6f} ({pct:.2f}% slower)")
            if fail_on_regression:
                sys.exit(1)
        else:
            print("✅ No regressions found")
        EOF

